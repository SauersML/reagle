diff --git a/Rust/src/data/storage/matrix.rs b/Rust/src/data/storage/matrix.rs
index 24971ab..8ff7a08 100644
--- a/Rust/src/data/storage/matrix.rs
+++ b/Rust/src/data/storage/matrix.rs
@@ -17,6 +17,66 @@ use crate::data::marker::{Marker, MarkerIdx, Markers};
 use crate::data::storage::phase_state::{PhaseState, Phased, Unphased};
 use crate::data::storage::GenotypeColumn;
 
+/// A view into a subset of markers in a GenotypeMatrix
+///
+/// Provides zero-copy access to a range of markers without cloning the underlying data.
+/// Marker indices in the view are relative (0-based), but global positions are preserved.
+#[derive(Debug)]
+pub struct GenotypeView<'a, State: PhaseState> {
+    /// Reference to the full matrix
+    matrix: &'a GenotypeMatrix<State>,
+    /// Start marker index in the full matrix
+    start: usize,
+    /// End marker index (exclusive) in the full matrix
+    end: usize,
+}
+
+impl<'a, State: PhaseState> GenotypeView<'a, State> {
+    /// Get the number of markers in this view
+    pub fn n_markers(&self) -> usize {
+        self.end - self.start
+    }
+
+    /// Get the number of haplotypes
+    pub fn n_haplotypes(&self) -> usize {
+        self.matrix.n_haplotypes()
+    }
+
+    /// Get allele for a marker/haplotype pair (relative indices)
+    pub fn allele(&self, marker: MarkerIdx, hap: HapIdx) -> u8 {
+        let global_marker = MarkerIdx::new((self.start + marker.as_usize()) as u32);
+        self.matrix.allele(global_marker, hap)
+    }
+
+    /// Get marker metadata (relative indices)
+    pub fn marker(&self, marker: MarkerIdx) -> &Marker {
+        let global_marker = MarkerIdx::new((self.start + marker.as_usize()) as u32);
+        self.matrix.marker(global_marker)
+    }
+
+    /// Get the marker offset (index of first marker in the full matrix)
+    ///
+    /// When using markers() to get metadata, add this offset to convert
+    /// from view-relative indices to matrix-global indices.
+    pub fn marker_offset(&self) -> usize {
+        self.start
+    }
+
+    /// Get markers metadata from the full matrix
+    ///
+    /// NOTE: This returns the full matrix's markers, not a subset.
+    /// Use `marker_offset()` to translate view-relative indices to global indices.
+    /// For example: `view.markers().marker(MarkerIdx::new((view.marker_offset() + local_idx) as u32))`
+    pub fn markers(&self) -> &Markers {
+        self.matrix.markers()
+    }
+
+    /// Get samples metadata
+    pub fn samples(&self) -> &Arc<Samples> {
+        self.matrix.samples()
+    }
+}
+
 /// The main genotype matrix structure.
 ///
 /// Type parameter `State` encodes whether data is phased at compile time,
@@ -126,6 +186,16 @@ impl<S: PhaseState> GenotypeMatrix<S> {
     pub fn confidence_clone(&self) -> Option<Vec<Vec<u8>>> {
         self.confidence.clone()
     }
+
+    /// Create a zero-copy view of a marker range
+    pub fn get_window_view(&self, range: std::ops::Range<usize>) -> GenotypeView<State> {
+        assert!(range.start <= range.end && range.end <= self.n_markers());
+        GenotypeView {
+            matrix: self,
+            start: range.start,
+            end: range.end,
+        }
+    }
 }
 
 // ============================================================================
diff --git a/Rust/src/io/bref3.rs b/Rust/src/io/bref3.rs
index 91b9dd1..d3fed62 100644
--- a/Rust/src/io/bref3.rs
+++ b/Rust/src/io/bref3.rs
@@ -365,7 +365,7 @@ impl Bref3Block {
 /// discard data as needed.
 pub struct StreamingBref3Reader {
     reader: BufReader<File>,
-    samples: Samples,
+    samples: Arc<Samples>,
     n_haps: usize,
     chrom_map: std::collections::HashMap<String, ChromIdx>,
     /// Whether we've reached end of data
@@ -390,7 +390,7 @@ impl StreamingBref3Reader {
         read_utf8_string(&mut reader)?; // program string (unused)
         let sample_ids = read_string_array(&mut reader)?;
         let n_haps = sample_ids.len() * 2;
-        let samples = Samples::from_ids(sample_ids);
+        let samples = Arc::new(Samples::from_ids(sample_ids));
 
         Ok(Self {
             reader,
@@ -406,6 +406,11 @@ impl StreamingBref3Reader {
         &self.samples
     }
 
+    /// Get the samples as Arc (for sharing without cloning)
+    pub fn samples_arc(&self) -> Arc<Samples> {
+        Arc::clone(&self.samples)
+    }
+
     /// Get number of haplotypes
     pub fn n_haps(&self) -> usize {
         self.n_haps
@@ -596,10 +601,8 @@ impl Default for WindowConfig {
 
 /// A window of reference data accumulated from multiple blocks
 pub struct RefWindow {
-    /// All markers in this window
-    pub markers: Markers,
-    /// All genotype columns
-    pub columns: Vec<GenotypeColumn>,
+    /// Genotype matrix for this window (phased reference data)
+    pub genotypes: GenotypeMatrix<crate::data::storage::phase_state::Phased>,
     /// Global start marker index
     pub global_start: usize,
     /// Global end marker index (exclusive)
@@ -648,6 +651,103 @@ impl WindowedBref3Reader {
         self.inner.n_haps()
     }
 
+    /// Load reference window for a specific genomic region
+    ///
+    /// This method loads all reference markers within [start_pos, end_pos].
+    /// Used to synchronize with target windows based on genomic coordinates
+    /// rather than independent window boundaries.
+    pub fn load_window_for_region(&mut self, start_pos: u32, end_pos: u32) -> Result<Option<RefWindow>> {
+        // First, drain any blocks that are entirely before start_pos
+        while let Some(first_block) = self.block_buffer.first() {
+            if first_block.end_pos < start_pos {
+                self.block_buffer.remove(0);
+            } else {
+                break;
+            }
+        }
+
+        // Load blocks until we cover end_pos
+        while !self.inner.is_eof() {
+            let need_more = self.block_buffer.is_empty()
+                || self.block_buffer.last().map(|b| b.end_pos < end_pos).unwrap_or(true);
+
+            if !need_more {
+                break;
+            }
+
+            if let Some(block) = self.inner.next_block()? {
+                // Skip blocks entirely before start_pos
+                if block.end_pos < start_pos {
+                    continue;
+                }
+                self.block_buffer.push(block);
+            } else {
+                break;
+            }
+        }
+
+        if self.block_buffer.is_empty() {
+            return Ok(None);
+        }
+
+        // Merge blocks that overlap with [start_pos, end_pos]
+        let mut markers = Markers::new();
+        let mut columns: Vec<GenotypeColumn> = Vec::new();
+        let is_first = self.window_num == 0;
+        let is_last = self.inner.is_eof();
+
+        for block in &self.block_buffer {
+            // Skip blocks entirely outside our range
+            if block.end_pos < start_pos || block.start_pos > end_pos {
+                continue;
+            }
+
+            // Add chromosome if needed
+            if markers.chrom_names().is_empty() || markers.chrom_names().last().map(|s| s.as_ref()) != Some(&block.chrom) {
+                markers.add_chrom(&block.chrom);
+            }
+
+            // Add markers within the position range
+            for m in 0..block.n_markers() {
+                let marker = block.markers.marker(crate::data::marker::MarkerIdx::new(m as u32));
+                if marker.pos >= start_pos && marker.pos <= end_pos {
+                    markers.push(marker.clone());
+                    columns.push(block.columns[m].clone());
+                }
+            }
+        }
+
+        if markers.is_empty() {
+            return Ok(None);
+        }
+
+        let n_markers = markers.len();
+        let global_start = self.global_offset;
+        let global_end = global_start + n_markers;
+
+        self.global_offset = global_end;
+        self.window_num += 1;
+
+        // Clear blocks we've processed (keep last one for potential overlap)
+        while self.block_buffer.len() > 1 && self.block_buffer[0].end_pos < end_pos {
+            self.block_buffer.remove(0);
+        }
+
+        // Create GenotypeMatrix from markers and columns
+        let samples = self.inner.samples_arc();
+        let genotypes = GenotypeMatrix::new_phased(markers, columns, samples);
+
+        Ok(Some(RefWindow {
+            genotypes,
+            global_start,
+            global_end,
+            output_start: 0,
+            output_end: n_markers,
+            is_first,
+            is_last,
+        }))
+    }
+
     /// Read the next window of reference data
     pub fn next_window(&mut self) -> Result<Option<RefWindow>> {
         // Fill buffer until we have enough for a window
@@ -745,9 +845,12 @@ impl WindowedBref3Reader {
         self.block_buffer.drain(..keep_from);
         self.window_num += 1;
 
+        // Create GenotypeMatrix from markers and columns
+        let samples = self.inner.samples_arc();
+        let genotypes = GenotypeMatrix::new_phased(markers, columns, samples);
+
         Ok(Some(RefWindow {
-            markers,
-            columns,
+            genotypes,
             global_start,
             global_end,
             output_start,
@@ -758,6 +861,143 @@ impl WindowedBref3Reader {
     }
 }
 
+/// Unified reference panel reader that supports both BREF3 (streaming) and VCF (in-memory)
+pub enum RefPanelReader {
+    /// Streaming BREF3 reader
+    Bref3(WindowedBref3Reader),
+    /// In-memory VCF reader
+    InMemory(InMemoryRefReader),
+}
+
+impl RefPanelReader {
+    /// Get the samples
+    pub fn samples(&self) -> &Samples {
+        match self {
+            RefPanelReader::Bref3(r) => r.samples(),
+            RefPanelReader::InMemory(r) => r.samples(),
+        }
+    }
+
+    /// Get the samples as Arc
+    pub fn samples_arc(&self) -> Arc<Samples> {
+        match self {
+            RefPanelReader::Bref3(r) => r.inner.samples_arc(),
+            RefPanelReader::InMemory(r) => r.samples_arc(),
+        }
+    }
+
+    /// Get number of haplotypes
+    pub fn n_haps(&self) -> usize {
+        match self {
+            RefPanelReader::Bref3(r) => r.n_haps(),
+            RefPanelReader::InMemory(r) => r.n_haps(),
+        }
+    }
+
+    /// Load reference window for a specific genomic region
+    pub fn load_window_for_region(&mut self, start_pos: u32, end_pos: u32) -> Result<Option<RefWindow>> {
+        match self {
+            RefPanelReader::Bref3(r) => r.load_window_for_region(start_pos, end_pos),
+            RefPanelReader::InMemory(r) => r.load_window_for_region(start_pos, end_pos),
+        }
+    }
+}
+
+/// In-memory reference panel reader for VCF files
+///
+/// Provides the same interface as WindowedBref3Reader but backed by
+/// an in-memory GenotypeMatrix. Used for VCF reference panels.
+pub struct InMemoryRefReader {
+    genotypes: Arc<GenotypeMatrix<crate::data::storage::phase_state::Phased>>,
+    window_num: usize,
+}
+
+impl InMemoryRefReader {
+    /// Create a new in-memory reader from a loaded reference panel
+    pub fn new(genotypes: Arc<GenotypeMatrix<crate::data::storage::phase_state::Phased>>) -> Self {
+        Self {
+            genotypes,
+            window_num: 0,
+        }
+    }
+
+    /// Get the samples
+    pub fn samples(&self) -> &Samples {
+        self.genotypes.samples()
+    }
+
+    /// Get the samples as Arc
+    pub fn samples_arc(&self) -> Arc<Samples> {
+        self.genotypes.samples_arc()
+    }
+
+    /// Get number of haplotypes
+    pub fn n_haps(&self) -> usize {
+        self.genotypes.n_haplotypes()
+    }
+
+    /// Load reference window for a specific genomic region
+    pub fn load_window_for_region(&mut self, start_pos: u32, end_pos: u32) -> Result<Option<RefWindow>> {
+        use crate::data::marker::MarkerIdx;
+
+        let n_markers = self.genotypes.n_markers();
+        if n_markers == 0 {
+            return Ok(None);
+        }
+
+        // Find markers within the position range
+        let mut start_idx = None;
+        let mut end_idx = None;
+
+        for m in 0..n_markers {
+            let marker = self.genotypes.marker(MarkerIdx::new(m as u32));
+            if marker.pos >= start_pos && marker.pos <= end_pos {
+                if start_idx.is_none() {
+                    start_idx = Some(m);
+                }
+                end_idx = Some(m + 1);
+            } else if marker.pos > end_pos {
+                break;
+            }
+        }
+
+        let (start_idx, end_idx) = match (start_idx, end_idx) {
+            (Some(s), Some(e)) => (s, e),
+            _ => return Ok(None),
+        };
+
+        // Extract markers and columns for this range
+        let mut markers = crate::data::marker::Markers::new();
+        let mut columns = Vec::new();
+
+        // Add chromosome
+        let first_marker = self.genotypes.marker(MarkerIdx::new(start_idx as u32));
+        let chrom_name = self.genotypes.markers().chrom_name(first_marker.chrom);
+        markers.add_chrom(chrom_name);
+
+        for m in start_idx..end_idx {
+            markers.push(self.genotypes.marker(MarkerIdx::new(m as u32)).clone());
+            columns.push(self.genotypes.column(m).clone());
+        }
+
+        let n_window_markers = end_idx - start_idx;
+        let is_first = self.window_num == 0;
+        self.window_num += 1;
+
+        let genotypes = GenotypeMatrix::new_phased(markers, columns, self.genotypes.samples_arc());
+
+        Ok(Some(RefWindow {
+            genotypes,
+            global_start: start_idx,
+            global_end: end_idx,
+            output_start: 0,
+            output_end: n_window_markers,
+            is_first,
+            is_last: false, // Can't know without looking ahead
+        }))
+    }
+}
+
 #[cfg(test)]
 mod tests {
     use super::*;
diff --git a/Rust/src/io/vcf.rs b/Rust/src/io/vcf.rs
index 90d24a6..ca257d7 100644
--- a/Rust/src/io/vcf.rs
+++ b/Rust/src/io/vcf.rs
@@ -12,6 +12,7 @@ use noodles::bgzf::io as bgzf_io;
 use noodles::vcf::Header;
 
 use crate::data::haplotype::Samples;
+use crate::pipelines::imputation::ImputationQuality;
 use crate::data::marker::{Allele, Marker, MarkerIdx, Markers};
 use crate::data::storage::{GenotypeColumn, GenotypeMatrix, PhaseState, compress_block};
 use crate::error::{ReagleError, Result};
@@ -1171,6 +1172,172 @@ impl VcfWriter {
         Ok(())
     }
 
+    /// Write imputed data for a single window
+    ///
+    /// Simplified version of write_imputed_streaming for windowed processing.
+    /// Takes closures that provide dosage and genotype data per marker.
+    pub fn write_imputed_window<S, F, B>(
+        &mut self,
+        matrix: &GenotypeMatrix<S>,
+        get_dosage: F,
+        get_best_gt: B,
+        get_posteriors: Option<Box<dyn Fn(usize, usize) -> (crate::pipelines::imputation::AllelePosteriors, crate::pipelines::imputation::AllelePosteriors)>>,
+        quality: &crate::pipelines::imputation::ImputationQuality,
+        start: usize,
+        end: usize,
+        include_gp: bool,
+        include_ap: bool,
+    ) -> Result<()>
+    where
+        S: PhaseState,
+        F: Fn(usize, usize) -> f32,
+        B: Fn(usize, usize) -> (u8, u8),
+    {
+        let n_samples = self.samples.len();
+
+        // Pre-compute format string
+        let format_str = {
+            let mut parts = vec!["GT", "DS"];
+            if include_gp { parts.push("GP"); }
+            if include_ap { parts.push("AP1"); parts.push("AP2"); }
+            parts.join(":")
+        };
+
+        let mut line_buf = String::with_capacity(n_samples * 50 + 200);
+        let mut ryu_buf = ryu::Buffer::new();
+
+        #[inline(always)]
+        fn format_f32_4dp(val: f32, ryu_buf: &mut ryu::Buffer) -> &str {
+            if !val.is_finite() {
+                return "0.0000";
+            }
+            let s = ryu_buf.format(val);
+            if let Some(dot_pos) = s.find('.') {
+                let end = (dot_pos + 5).min(s.len());
+                &s[..end]
+            } else {
+                s
+            }
+        }
+
+        for m in start..end {
+            line_buf.clear();
+            let marker_idx = MarkerIdx::new(m as u32);
+            let marker = matrix.marker(marker_idx);
+            let n_alleles = 1 + marker.alt_alleles.len();
+
+            // Build INFO field
+            let info_field = if let Some(stats) = quality.get(m) {
+                let mut info_str = String::with_capacity(64);
+                if n_alleles > 1 {
+                    info_str.push_str("DR2=");
+                    for a in1..n_alleles {
+                        if a > 1 { info_str.push(','); }
+                        info_str.push_str(format_f32_4dp(stats.dr2(a) as f32, &mut ryu_buf));
+                    }
+                    info_str.push_str(";AF=");
+                    for a in1..n_alleles {
+                        if a > 1 { info_str.push(','); }
+                        info_str.push_str(format_f32_4dp(stats.allele_freq(a) as f32, &mut ryu_buf));
+                    }
+                }
+                if stats.is_imputed {
+                    if !info_str.is_empty() { info_str.push(';'); }
+                    info_str.push_str("IMP");
+                }
+                if info_str.is_empty() { ".".to_string() } else { info_str }
+            } else { ".".to_string() };
+
+            use std::fmt::Write;
+            write!(line_buf, "{}\t{}\t{}\t{}\t.\tPASS\t{}\t{}",
+                matrix.markers().chrom_name(marker.chrom).unwrap_or("."),
+                marker.pos,
+                marker.id.as_ref().map(|s| s.as_ref()).unwrap_or("."),
+                marker.ref_allele,
+                marker.alt_alleles.iter().map(|a| a.to_string()).collect::<Vec<_>>().join(","),
+                info_field, format_str).unwrap();
+
+            for s in 0..n_samples {
+                let ds = get_dosage(m, s);
+                let posteriors = get_posteriors.as_ref().map(|f| f(m, s));
+                let (a1, a2) = if let Some((ref p1, ref p2)) = posteriors {
+                    (p1.max_allele(), p2.max_allele())
+                } else { get_best_gt(m, s) };
+
+                line_buf.push('\t');
+                line_buf.push((b'0' + a1) as char);
+                line_buf.push('|');
+                line_buf.push((b'0' + a2) as char);
+                line_buf.push(':');
+                line_buf.push_str(format_f32_4dp(ds, &mut ryu_buf));
+
+                if include_gp {
+                    line_buf.push(':');
+                    if let Some((ref p1, ref p2)) = posteriors {
+                        for a in 1..n_alleles {
+                            if a > 1 { line_buf.push(','); }
+                            line_buf.push_str(format_f32_4dp(p1.prob(a), &mut ryu_buf));
+                        }
+                        if n_alleles <= 1 { line_buf.push_str("0.00"); }
+                        line_buf.push(':');
+                        for a in 1..n_alleles {
+                            if a > 1 { line_buf.push(','); }
+                            line_buf.push_str(format_f32_4dp(p2.prob(a), &mut ryu_buf));
+                        }
+                        if n_alleles <= 1 { line_buf.push_str("0.00"); }
+                    } else {
+                        let n_gp = n_alleles * (n_alleles + 1) / 2;
+                        for _ in 0..n_gp {
+                            if line_buf.ends_with(':') || line_buf.ends_with('\t') {
+                                line_buf.push_str("0.00");
+                            } else {
+                                line_buf.push_str(",0.00");
+                            }
+                        }
+                    }
+                }
+
+                if include_ap {
+                    line_buf.push(':');
+                    if let Some((ref p1, ref p2)) = posteriors {
+                        for a in 1..n_alleles {
+                            if a > 1 { line_buf.push(','); }
+                            line_buf.push_str(format_f32_4dp(p1.prob(a), &mut ryu_buf));
+                        }
+                        if n_alleles <= 1 { line_buf.push_str("0.00"); }
+                        line_buf.push(':');
+                        for a in 1..n_alleles {
+                            if a > 1 { line_buf.push(','); }
+                            line_buf.push_str(format_f32_4dp(p2.prob(a), &mut ryu_buf));
+                        }
+                        if n_alleles <= 1 { line_buf.push_str("0.00"); }
+                    } else {
+                        let n_ap = n_alleles.saturating_sub(1).max(1);
+                        for _ in 0..n_ap {
+                            if line_buf.ends_with(':') || line_buf.ends_with('\t') {
+                                line_buf.push_str("0.00");
+                            } else {
+                                line_buf.push_str(",0.00");
+                            }
+                        }
+                        line_buf.push(':');
+                        for _ in 0..n_ap {
+                            if line_buf.ends_with(':') || line_buf.ends_with('\t') {
+                                line_buf.push_str("0.00");
+                            } else {
+                                line_buf.push_str(",0.00");
+                            }
+                        }
+                    }
+                }
+            }
+
+            line_buf.push('\n');
+            self.writer.write_all(line_buf.as_bytes())?;
+        }
+        Ok(())
+    }
+
     /// Flush the writer
     pub fn flush(&mut self) -> Result<()> {
         self.writer.flush()?;
diff --git a/Rust/src/model/pbwt.rs b/Rust/src/model/pbwt.rs
index 7fc3b6b..b2afb39 100644
--- a/Rust/src/model/pbwt.rs
+++ b/Rust/src/model/pbwt.rs
@@ -15,6 +15,40 @@
 //! Durbin, Richard (2014) Efficient haplotype matching and storage using the
 //! positional Burrows-Wheeler transform (PBWT).
 
+/// Snapshot of PBWT state for window handoff
+///
+/// Contains the prefix array and divergence array at a specific marker position.
+/// Used to initialize PBWT in the next window without recomputation.
+#[derive(Debug, Clone)]
+pub struct PbwtState {
+    /// Prefix array (ppa): current haplotype sort order
+    pub ppa: Vec<u32>,
+    /// Divergence array: positions where haplotypes diverge
+    pub div: Vec<i32>,
+    /// Marker position this state corresponds to
+    pub marker_pos: usize,
+}
+
+impl PbwtState {
+    /// Create a new PBWT state snapshot
+    pub fn new(ppa: Vec<u32>, div: Vec<i32>, marker_pos: usize) -> Self {
+        Self {
+            ppa,
+            div,
+            marker_pos,
+        }
+    }
+
+    /// Extract state from a PbwtDivUpdater at a given marker
+    pub fn from_updater(updater: &PbwtDivUpdater, marker_pos: usize) -> Self {
+        Self {
+            ppa: updater.a.clone(),
+            div: updater.d.clone(),
+            marker_pos,
+        }
+    }
+}
+
 /// PBWT updater with divergence array tracking
 ///
 /// This optimized implementation uses flat arrays and Counting Sort
diff --git a/Rust/src/pipelines/imputation.rs b/Rust/src/pipelines/imputation.rs
index 8f26766..cc0b02d 100644
--- a/Rust/src/pipelines/imputation.rs
+++ b/Rust/src/pipelines/imputation.rs
@@ -620,6 +620,76 @@ pub struct StateProbs {
     marker_to_cluster: std::sync::Arc<Vec<usize>>,
 }
 
+impl MarkerAlignment {
+    /// Create alignment from windows for streaming imputation
+    ///
+    /// This is a lightweight version that aligns only the markers in the
+    /// current window, avoiding the O(M) alignment cost.
+    pub fn new_from_windows(
+        target_win: &GenotypeMatrix,
+        ref_win: &GenotypeMatrix<Phased>,
+        ref_pos_map: &std::collections::HashMap<(u32, u32), usize>,
+    ) -> Result<Self> {
+        let n_ref_markers = ref_win.n_markers();
+        let n_target_markers = target_win.n_markers();
+
+        // Build position -> target index map for this window
+        let mut target_pos_map: std::collections::HashMap<(u16, u32), usize> = std::collections::HashMap::new();
+        for m in 0..n_target_markers {
+            let marker = target_win.marker(crate::data::marker::MarkerIdx::new(m as u32));
+            target_pos_map.insert((marker.chrom.0, marker.pos), m);
+        }
+
+        // Map reference markers to target markers
+        let mut ref_to_target = vec![-1i32; n_ref_markers];
+        let mut target_to_ref = vec![0usize; n_target_markers];
+        let mut allele_mappings: Vec<Option<crate::data::marker::AlleleMapping>> =
+            vec![None; n_target_markers];
+
+        for ref_m in 0..n_ref_markers {
+            let ref_marker = ref_win.marker(crate::data::marker::MarkerIdx::new(ref_m as u32));
+            let ref_pos_global = (ref_marker.chrom.0 as u32, ref_marker.pos);
+
+            // Check if this reference marker is genotyped in target window
+            if let Some(&target_idx) = target_pos_map.get(&ref_pos_global) {
+                let target_marker = target_win.marker(crate::data::marker::MarkerIdx::new(target_idx as u32));
+
+                // Compute allele mapping
+                if let Some(mapping) = crate::data::marker::compute_allele_mapping(target_marker, ref_marker) {
+                    if mapping.is_valid() {
+                        ref_to_target[ref_m] = target_idx as i32;
+                        target_to_ref[target_idx] = ref_m;
+                        allele_mappings[target_idx] = Some(mapping);
+                    }
+                }
+            }
+        }
+
+        Ok(Self {
+            ref_to_target,
+            target_to_ref,
+            allele_mappings,
+        })
+    }
+
+    /// Create alignment from reference panel and position map
+    ///
+    /// Used for initializing alignment before processing windows.
+    pub fn new_from_position_map(
+        samples: &crate::data::haplotype::Samples,
+        ref_pos_map: &std::collections::HashMap<(u32, u32), usize>,
+    ) -> Result<Self> {
+        // For streaming imputation, we don't have all target genotypes at this point
+        // This is a stub that gets filled in by new_from_windows later
+        // Return empty alignment for now
+        Ok(Self {
+            ref_to_target: vec![],
+            target_to_ref: vec![],
+            allele_mappings: vec![],
+        })
+    }
+}
+
 #[cfg(test)]
 impl StateProbs {
     /// Create state probabilities from sparse HMM output.
@@ -1287,252 +1357,411 @@ impl ImputationPipeline {
     /// Run the imputation pipeline
     #[instrument(name = "imputation", skip(self))]
     pub fn run(&mut self) -> Result<()> {
-        let (target_reader, target_gt, target_samples) = info_span!("load_target").in_scope(|| {
-            eprintln!("Loading target VCF...");
-            let (mut target_reader, target_file) = VcfReader::open(&self.config.gt)?;
-            let target_samples = target_reader.samples_arc();
-            let target_gt = target_reader.read_all(target_file)?;
-            Ok::<_, crate::error::ReagleError>((target_reader, target_gt, target_samples))
-        })?;
+        // Configure streaming
+        let streaming_config = crate::io::streaming::StreamingConfig {
+            window_cm: self.config.window,
+            overlap_cm: self.config.overlap,
+            ..Default::default()
+        };
 
-        let ref_gt: Arc<GenotypeMatrix<Phased>> = info_span!("load_reference").in_scope(|| {
-            eprintln!("Loading reference panel...");
-            let ref_path = self.config.r#ref.as_ref().ok_or_else(|| {
-                crate::error::ReagleError::config("Reference panel required for imputation")
-            })?;
-
-            // Detect file format by extension and load accordingly
-            Ok::<_, crate::error::ReagleError>(Arc::new(if ref_path.extension().map(|e| e == "bref3").unwrap_or(false) {
-                eprintln!("  Detected BREF3 format (streaming)");
-                // Use streaming loader for memory-efficient loading
-                Self::load_reference_streaming(ref_path, None)?
-            } else {
-                eprintln!("  Detected VCF format");
-                let (mut ref_reader, ref_file) = VcfReader::open(ref_path)?;
-                ref_reader.read_all(ref_file)?.into_phased()
-            }))
-        })?;
+        // Load genetic maps
+        let gen_maps = if let Some(ref map_path) = self.config.map {
+            GeneticMaps::from_plink_file(map_path, &[])?  // Empty chrom list for now
+        } else {
+            GeneticMaps::new()
+        };
 
-        if target_gt.n_markers() == 0 || ref_gt.n_markers() == 0 {
-            return Ok(());
-        }
+        // Open streaming readers
+        let mut target_reader = crate::io::streaming::StreamingVcfReader::open(&self.config.gt, gen_maps.clone(), streaming_config.clone())?;
+        let target_samples = target_reader.samples_arc();
 
-        // Create marker alignment (reused for both phasing and imputation)
-        let alignment = info_span!("align_markers").in_scope(|| {
-            eprintln!("Aligning markers...");
-            MarkerAlignment::new(&target_gt, &ref_gt)
-        });
+        let ref_path = self.config.r#ref.as_ref().ok_or_else(|| {
+            crate::error::ReagleError::config("Reference panel required for imputation")
+        })?;
 
-        let n_ref_haps = ref_gt.n_haplotypes();
-        let n_ref_markers = ref_gt.n_markers();
-        let n_total_haps = n_ref_haps + target_gt.n_haplotypes();
-        self.params = ModelParams::for_imputation(n_ref_haps, n_total_haps, self.config.ne, self.config.err);
-        let n_genotyped = alignment.ref_to_target.iter().filter(|&&x| x >= 0).count();
-        eprintln!(
-            "  {} of {} reference markers are genotyped in target",
-            n_genotyped, n_ref_markers
-        );
-        if target_gt.has_confidence() {
-            // Count how many marker-sample pairs have low confidence
-            let mut low_confidence_count = 0usize;
-            let mut total_count = 0usize;
-            for m in 0..target_gt.n_markers() {
-                for s in 0..target_gt.n_samples() {
-                    total_count += 1;
-                    if target_gt.sample_confidence_f32(MarkerIdx::new(m as u32), s) < 0.5 {
-                        low_confidence_count += 1;
-                    }
+        // Load reference panel based on format
+        let is_bref3 = ref_path.extension().map(|e| e == "bref3").unwrap_or(false);
+        let (mut ref_reader, ref_pos_map) = if is_bref3 {
+            // BREF3: Use streaming reader
+            eprintln!("Loading reference panel (BREF3 streaming)...");
+
+            // Build position map first
+            let map_reader = crate::io::bref3::StreamingBref3Reader::open(ref_path)?;
+            let mut pos_map: std::collections::HashMap<(u32, u32), usize> = std::collections::HashMap::new();
+            let mut global_idx = 0;
+            while let Some(block) = map_reader.next_block()? {
+                for marker in block.markers.iter() {
+                    pos_map.insert((marker.chrom.into(), marker.pos), global_idx);
+                    global_idx += 1;
                 }
             }
-            eprintln!(
-                "  Target has genotype confidence scores: {}/{} ({:.1}%) are low-confidence",
-                low_confidence_count, total_count,
-                100.0 * low_confidence_count as f64 / total_count.max(1) as f64
-            );
-        }
 
-        // Check if target data is already phased - skip phasing if so
-        let phased_target_gt_res: Result<GenotypeMatrix<Phased>> = if target_reader.was_all_phased() {
-            eprintln!("Target data is already phased, skipping phasing step");
-            Ok(target_gt.into_phased())
+            // Create streaming reader
+            let stream_reader = crate::io::bref3::StreamingBref3Reader::open(ref_path)?;
+            let window_config = crate::io::bref3::WindowConfig::default();
+            let windowed = crate::io::bref3::WindowedBref3Reader::new(stream_reader, window_config);
+
+            (crate::io::bref3::RefPanelReader::Bref3(windowed), pos_map)
         } else {
-            info_span!("phasing").in_scope(|| {
-                // Phase target before imputation (imputation requires phased haplotypes)
-                eprintln!("Phasing target data before imputation...");
-
-                // Create phasing pipeline with current config
-                let mut phasing = super::phasing::PhasingPipeline::new(self.config.clone());
-
-                // Set reference panel for reference-guided phasing
-                phasing.set_reference(Arc::clone(&ref_gt), alignment.clone());
-                eprintln!("Using reference panel ({} haplotypes) for phasing", n_ref_haps);
-
-                // Load genetic map if provided
-                let gen_maps = if let Some(ref map_path) = self.config.map {
-                    let chrom_names: Vec<&str> = target_gt
-                        .markers()
-                        .chrom_names()
-                        .iter()
-                        .map(|s| s.as_ref())
-                        .collect();
-                    GeneticMaps::from_plink_file(map_path, &chrom_names)?
-                } else {
-                    GeneticMaps::new()
-                };
+            // VCF: Load into memory
+            eprintln!("Loading reference panel (VCF in-memory)...");
+            let (mut vcf_reader, vcf_file) = VcfReader::open(ref_path)?;
+            let ref_gt = Arc::new(vcf_reader.read_all(vcf_file)?.into_phased());
+
+            // Build position map from loaded data
+            let mut pos_map: std::collections::HashMap<(u32, u32), usize> = std::collections::HashMap::new();
+            for m in 0..ref_gt.n_markers() {
+                let marker = ref_gt.marker(MarkerIdx::new(m as u32));
+                pos_map.insert((marker.chrom.into(), marker.pos), m);
+            }
 
-                phasing.phase_in_memory(&target_gt, &gen_maps)
-            })
+            let in_memory = crate::io::bref3::InMemoryRefReader::new(ref_gt);
+            (crate::io::bref3::RefPanelReader::InMemory(in_memory), pos_map)
         };
-        let target_gt = Arc::new(phased_target_gt_res?);
 
-        // Wrap large data structures in Arc for sharing with closures
-        let target_gt = Arc::new(target_gt);
-        let alignment = Arc::new(alignment);
-
-        let n_target_markers = target_gt.n_markers();
-        let n_target_samples = target_gt.n_samples();
-        let n_target_haps = target_gt.n_haplotypes();
-
-        eprintln!(
-            "Target: {} markers, {} samples; Reference: {} markers, {} haplotypes",
-            n_target_markers, n_target_samples, n_ref_markers, n_ref_haps
+        // Synchronized streaming imputation: O(W) memory instead of O(M)
+        // Process windows by position, write to VCF immediately after each window
+        let mut window_count = 0;
+        let mut total_markers = 0;
+
+        // Get reference panel info
+        let n_ref_haps = ref_reader.n_haps();
+        let n_target_samples = target_reader.samples().n_samples();
+        let n_target_haps = n_target_samples * 2;
+
+        // Initialize parameters
+        self.params = ModelParams::for_imputation(
+            n_ref_haps,
+            n_ref_haps + n_target_haps,
+            self.config.ne,
+            self.config.err,
         );
-
-        // Initialize parameters with CLI config
-        // Java uses different hap counts for different parameters:
-        // - errProb: uses nHaps = nRefHaps + nTargHaps (total)
-        // - pRecomb: uses refGT.nHaps() (ref only)
-        let n_total_haps = n_ref_haps + n_target_haps;
-        self.params = ModelParams::for_imputation(n_ref_haps, n_total_haps, self.config.ne, self.config.err);
-        self.params
-            .set_n_states(self.config.imp_states.min(n_ref_haps));
+        self.params.set_n_states(self.config.imp_states.min(n_ref_haps));
 
         // Load genetic map if provided
         let gen_maps = if let Some(ref map_path) = self.config.map {
-            let chrom_names: Vec<&str> = ref_gt
-                .markers()
-                .chrom_names()
-                .iter()
-                .map(|s| s.as_ref())
-                .collect();
-            GeneticMaps::from_plink_file(map_path, &chrom_names)?
+            GeneticMaps::from_plink_file(map_path, &[])?  // Empty for now
         } else {
             GeneticMaps::new()
         };
 
-        let chrom = ref_gt.marker(MarkerIdx::new(0)).chrom;
+        eprintln!(
+            "Streaming imputation: {} ref haplotypes, {} target samples",
+            n_ref_haps, n_target_samples
+        );
 
-        // Note: gen_positions and steps_config removed - ImpStates now uses ref_panel step boundaries directly
+        // Create output VCF writer
+        let output_path = self.config.out.with_extension("vcf.gz");
+        eprintln!("Writing output to {:?}", output_path);
+        let mut writer = VcfWriter::create(&output_path, target_reader.samples().clone())?;
 
-        // Build target-aligned marker list (Java uses all target markers, regardless of missingness).
-        let genotyped_markers_vec: Vec<usize> = (0..n_ref_markers)
-            .filter(|&ref_m| alignment.target_marker(ref_m).is_some())
-            .collect();
-        let n_genotyped = genotyped_markers_vec.len();
-        let n_to_impute = n_ref_markers - n_genotyped;
-        let has_observed: std::sync::Arc<Vec<bool>> = {
-            let mut flags = vec![false; n_ref_markers];
-            for &m in &genotyped_markers_vec {
-                flags[m] = true;
-            }
-            std::sync::Arc::new(flags)
-        };
+        // Thread-local workspace for parallel HMM
+        thread_local! {
+            static IMP_WORKSPACE: std::cell::RefCell<Option<crate::utils::workspace::ThreadWorkspace>> =
+                std::cell::RefCell::new(None);
+        }
 
-        // Number of IBS haplotypes to find per step (Java ImpIbs)
-        // Java: nHapsPerStep = imp_states / (imp_segment / imp_step)
-        let imp_states = self.params.n_states;
-        let n_steps_per_segment = (self.config.imp_segment / self.config.imp_step).round() as usize;
-        let n_steps_per_segment = n_steps_per_segment.max(1);
-        let n_ibs_haps = if n_ref_haps <= 1000 {
-            n_ref_haps
-        } else {
-            (imp_states / n_steps_per_segment)
-                .max(1)
-                .min(n_ref_haps)
-        };
+        // Process windows synchronized by genomic position
+        // Target window drives the position bounds, reference loads matching region
+        while let Some(t_win) = target_reader.next_window()? {
+            window_count += 1;
+            let n_markers = t_win.genotypes.n_markers();
 
-        // Cluster-coded haplotype sequences and recursive IBS matching (Java ImpIbs)
-
-        eprintln!("Running imputation with dynamic state selection...");
-        let n_states = self.params.n_states;
-
-        // Compute cumulative genetic positions for ALL reference markers
-        // Wrapped in Arc to share across all haplotypes without cloning
-        let gen_positions: std::sync::Arc<Vec<f64>> = {
-            let mut positions = Vec::with_capacity(n_ref_markers);
-            let mut cumulative = 0.0f64;
-            positions.push(0.0);
-            for m in 1..n_ref_markers {
-                let pos1 = ref_gt.marker(MarkerIdx::new((m - 1) as u32)).pos;
-                let pos2 = ref_gt.marker(MarkerIdx::new(m as u32)).pos;
-                let gen_dist = gen_maps.gen_dist(chrom, pos1, pos2);
-                cumulative += gen_dist.abs().max(MIN_CM_DIST);
-                positions.push(cumulative);
-            }
-            std::sync::Arc::new(positions)
-        };
+            if n_markers == 0 {
+                continue;
+            }
 
-        // Compute marker clusters per sample to avoid cross-sample leakage.
-        let cluster_dist = self.config.cluster as f64;
-        let base_err_rate = self.params.p_mismatch;
+            // Get position bounds from target window
+            let first_marker = t_win.genotypes.marker(crate::data::marker::MarkerIdx::new(0));
+            let last_marker = t_win.genotypes.marker(crate::data::marker::MarkerIdx::new((n_markers - 1) as u32));
+            let start_pos = first_marker.pos;
+            let end_pos = last_marker.pos;
 
-        eprintln!(
-            "  HMM on per-sample clusters ({} genotyped markers), interpolating {} ungenotyped",
-            n_genotyped, n_to_impute
-        );
+            eprintln!(
+                "Processing window {} ({} markers, pos {}..{}, global {}..{})",
+                window_count, n_markers, start_pos, end_pos,
+                t_win.global_start, t_win.global_end
+            );
 
-        // Initialize quality stats for all reference markers
-        let n_alleles_per_marker: Vec<usize> = (0..n_ref_markers)
-            .map(|m| {
-                let marker = ref_gt.marker(MarkerIdx::new(m as u32));
-                1 + marker.alt_alleles.len()
-            })
-            .collect();
-        let mut quality = ImputationQuality::new(&n_alleles_per_marker);
+            // Load reference window for the same genomic region
+            let r_win = match ref_reader.load_window_for_region(start_pos, end_pos)? {
+                Some(w) => w,
+                None => {
+                    eprintln!("  Warning: No reference markers in region {}..{}", start_pos, end_pos);
+                    continue;
+                }
+            };
 
-        // Mark imputed markers (those not in target)
-        for m in 0..n_ref_markers {
-            let is_imputed = !has_observed[m];
-            quality.set_imputed(m, is_imputed);
-        }
-
-        // Check if we need per-haplotype allele probabilities for AP/GP output
-        let need_allele_probs = self.config.ap || self.config.gp;
-
-        // Disk-buffered storage: write sample-major (sequential, fast), read chunked for output.
-        // This avoids the ~5-12 GB RAM buffer that would cause OOM on small machines.
-        // Layout: sample-major = [S0_M0, S0_M1, ..., S0_Mn, S1_M0, S1_M1, ...]
-        // Sequential writes during processing, chunked transpose during output.
-
-        let temp_dir = std::env::temp_dir();
-        let pid = std::process::id();
-        let dosage_path = temp_dir.join(format!("reagle_{}_dosages.tmp", pid));
-        let best_gt_path = temp_dir.join(format!("reagle_{}_gt.tmp", pid));
-        let posteriors_path = temp_dir.join(format!("reagle_{}_post.tmp", pid));
-
-        // Create temp files with pre-allocated size for sequential writes
-        use std::io::Write as IoWrite;
-        let dosage_file = std::fs::File::create(&dosage_path)?;
-        dosage_file.set_len((n_target_samples * n_ref_markers * 4) as u64)?;
-        let mut dosage_writer = std::io::BufWriter::with_capacity(8 * 1024 * 1024, dosage_file);
-
-        let gt_file = std::fs::File::create(&best_gt_path)?;
-        gt_file.set_len((n_target_samples * n_ref_markers * 2) as u64)?;
-        let mut gt_writer = std::io::BufWriter::with_capacity(4 * 1024 * 1024, gt_file);
-
-        let mut post_writer: Option<std::io::BufWriter<std::fs::File>> = if need_allele_probs {
-            let post_file = std::fs::File::create(&posteriors_path)?;
-            post_file.set_len((n_target_samples * n_ref_markers * 8) as u64)?;
-            Some(std::io::BufWriter::with_capacity(8 * 1024 * 1024, post_file))
-        } else {
-            None
-        };
+            // Local marker alignment for this window
+            let local_alignment = MarkerAlignment::new_from_windows(
+                &t_win.genotypes,
+                &r_win.genotypes,
+                &ref_pos_map,
+            )?;
+
+            let n_ref_markers_win = r_win.genotypes.n_markers();
+
+            // Build ImpStates for this window
+            let imp_states_obj = ImpStates::new(
+                &r_win.genotypes,
+                self.config.imp_segment,
+                self.config.imp_step,
+                self.config.imp_states,
+                1,  // n_steps_per_segment
+                self.config.imp_states,  // n_ibs_haps
+                self.params.p_mismatch,
+                self.config.cluster,
+                self.params.p_recomb,
+                &gen_maps,
+                Some(local_alignment.clone()),
+            )?;
+
+            // Initialize quality stats for window
+            let n_alleles_per_marker: Vec<usize> = (0..n_ref_markers_win)
+                .map(|m| {
+                    let marker = r_win.genotypes.marker(crate::data::marker::MarkerIdx::new(m as u32));
+                    1 + marker.alt_alleles.len()
+                })
+                .collect();
+            let mut quality = ImputationQuality::new(&n_alleles_per_marker);
+
+            // Mark imputed markers (those not in target)
+            let has_observed: Vec<bool> = (0..n_ref_markers_win)
+                .map(|ref_m| local_alignment.target_marker(ref_m).is_some())
+                .collect();
+
+            for m in 0..n_ref_markers_win {
+                quality.set_imputed(m, !has_observed[m]);
+            }
+
+            // Check if we need per-haplotype allele probabilities for AP/GP output
+            let need_allele_probs = self.config.ap || self.config.gp;
+
+            // Windowed batch size for parallel processing
+            const WINDOW_BATCH_SIZE: usize = 10;  // Reduced for memory efficiency
+
+            info_span!("process_window", n_samples = n_target_samples, batch_size = WINDOW_BATCH_SIZE).in_scope(|| {
+                let n_batches = (n_target_samples + WINDOW_BATCH_SIZE - 1) / WINDOW_BATCH_SIZE;
+
+                // Collect all results in this window
+                let window_results: std::sync::Mutex<Vec<(usize, Vec<f32>, Vec<(u8, u8)>, Option<Vec<(f32, f32)>>)>> =
+                    std::sync::Mutex::new(Vec::new());
+
+                for batch_idx in 0..n_batches {
+                    let batch_start = batch_idx * WINDOW_BATCH_SIZE;
+                    let batch_end = (batch_start + WINDOW_BATCH_SIZE).min(n_target_samples);
+                    let batch_samples: Vec<usize> = (batch_start..batch_end).collect();
+
+                    // Parallel HMM computation for this batch in the window
+                    batch_samples
+                        .par_iter()
+                        .for_each(|&s| {
+                            let hap1_idx = HapIdx::new((s * 2) as u32);
+                            let hap2_idx = HapIdx::new((s * 2 + 1) as u32);
+                            let target_haps = [hap1_idx, hap2_idx];
+
+                            // Get sample sequences from target window
+                            let seq1: Vec<u8> = (0..n_markers)
+                                .map(|m| t_win.genotypes.allele(
+                                    crate::data::marker::MarkerIdx::new(m as u32),
+                                    hap1_idx,
+                                ))
+                                .collect();
+                            let seq2: Vec<u8> = (0..n_markers)
+                                .map(|m| t_win.genotypes.allele(
+                                    crate::data::marker::MarkerIdx::new(m as u32),
+                                    hap2_idx,
+                                ))
+                                .collect();
+
+                            // Determine genotyped markers for this sample in this window
+                            let sample_genotyped: Vec<usize> = (0..n_ref_markers_win)
+                                .filter(|&ref_m| {
+                                    if let Some(target_m) = local_alignment.target_marker(ref_m) {
+                                        let marker_idx = crate::data::marker::MarkerIdx::new(target_m as u32);
+                                        let a1 = t_win.genotypes.allele(marker_idx, hap1_idx);
+                                        let a2 = t_win.genotypes.allele(marker_idx, hap2_idx);
+                                        a1 != 255 || a2 != 255
+                                    } else {
+                                        false
+                                    }
+                                })
+                                .collect();
+
+                            // Use thread-local workspace to avoid repeated allocations
+                            let dosages_and_gt = IMP_WORKSPACE.with(|ws| {
+                                let mut workspace = ws.borrow_mut();
+                                if workspace.is_none() {
+                                    *workspace = Some(crate::utils::workspace::ThreadWorkspace::new(0, 0));
+                                }
+                                let ws = workspace.as_mut().unwrap();
+                                ws.resize_for_window(n_ref_markers_win, self.config.imp_states);
+
+                                // Compute HMM state probabilities using ImpStates
+                                let state_probs = imp_states_obj.compute_state_probs(
+                                    &seq1,
+                                    &seq2,
+                                    ws,
+                                    &self.params,
+                                );
 
-        let max_alleles = n_alleles_per_marker.iter().copied().max().unwrap_or(2);
+                                // Compute dosages and genotypes from state probabilities
+                                let mut dosages = Vec::with_capacity(n_ref_markers_win);
+                                let mut best_gt = Vec::with_capacity(n_ref_markers_win);
+                                let mut posteriors = if need_allele_probs {
+                                    Some(Vec::with_capacity(n_ref_markers_win))
+                                } else {
+                                    None
+                                };
 
-        // Batch size for parallel processing - balances parallelism vs memory
-        // Each batch holds state_probs temporarily, so batch_size * per_sample_state_probs should fit in RAM
-        const BATCH_SIZE: usize = 50;
+                                for (ref_m, probs) in state_probs.iter().enumerate() {
+                                    let n_alleles = n_alleles_per_marker[ref_m];
+                                    let is_genotyped = has_observed[ref_m];
+
+                                    // Get observed genotypes if available
+                                    let (observed_a1, observed_a2) = if is_genotyped {
+                                        if let Some(target_m) = local_alignment.target_marker(ref_m) {
+                                            let marker_idx = crate::data::marker::MarkerIdx::new(target_m as u32);
+                                            (
+                                                t_win.genotypes.allele(marker_idx, hap1_idx),
+                                                t_win.genotypes.allele(marker_idx, hap2_idx),
+                                            )
+                                        } else {
+                                            (255, 255)
+                                        }
+                                    } else {
+                                        (255, 255)
+                                    };
+
+                                    let (use_observed_a1, use_observed_a2) = (
+                                        is_genotyped && observed_a1 != 255,
+                                        is_genotyped && observed_a2 != 255,
+                                    );
+
+                                    // Compute dosage and genotype
+                                    let probs1 = &probs[0];
+                                    let probs2 = &probs[1];
+
+                                    let d1 = if use_observed_a1 {
+                                        observed_a1 as f32
+                                    } else if n_alleles == 2 {
+                                        probs1[1]
+                                    } else {
+                                        (1..n_alleles).map(|a| a as f32 * probs1[a]).sum::<f32>()
+                                    };
+                                    let d2 = if use_observed_a2 {
+                                        observed_a2 as f32
+                                    } else if n_alleles == 2 {
+                                        probs2[1]
+                                    } else {
+                                        (1..n_alleles).map(|a| a as f32 * probs2[a]).sum::<f32>()
+                                    };
+                                    dosages.push(d1 + d2);
+
+                                    let best_a1 = if use_observed_a1 {
+                                        observed_a1
+                                    } else if n_alleles == 2 {
+                                        if probs1[1] > probs1[0] { 1 } else { 0 }
+                                    } else {
+                                        (0..n_alleles).max_by(|&a, &b| probs1[a].partial_cmp(&probs1[b]).unwrap_or(std::cmp::Ordering::Equal)).unwrap_or(0) as u8
+                                    };
+                                    let best_a2 = if use_observed_a2 {
+                                        observed_a2
+                                    } else if n_alleles == 2 {
+                                        if probs2[1] > probs2[0] { 1 } else { 0 }
+                                    } else {
+                                        (0..n_alleles).max_by(|&a, &b| probs2[a].partial_cmp(&probs2[b]).unwrap_or(std::cmp::Ordering::Equal)).unwrap_or(0) as u8
+                                    };
+                                    best_gt.push((best_a1, best_a2));
+
+                                    if let Some(ref mut post_vec) = posteriors {
+                                        post_vec.push((
+                                            if use_observed_a1 { observed_a1 as f32 } else { probs1.get(1).copied().unwrap_or(0.0) },
+                                            if use_observed_a2 { observed_a2 as f32 } else { probs2.get(1).copied().unwrap_or(0.0) },
+                                        ));
+                                    }
+                                }
+
+                                (dosages, best_gt, posteriors)
+                            });
+
+                            // Add to window results
+                            let mut results = window_results.lock().unwrap();
+                            results.push((s, dosages_and_gt.0, dosages_and_gt.1, dosages_and_gt.2));
+                        });
+                }
+
+                // Sort results by sample index
+                let mut sorted_results = window_results.into_inner();
+                sorted_results.sort_by_key(|(s, _, _, _)| *s);
+
+                // In-memory transpose: convert sample-major to marker-major for VCF writing
+                // For a window: transposed[m][s] = results[s][m]
+                let n_samples_batch = sorted_results.len();
+                let transposed_dosages: Vec<Vec<f32>> = (0..n_ref_markers_win)
+                    .map(|m| sorted_results.iter().map(|(_, d, _, _)| d[m]).collect())
+                    .collect();
+                let transposed_gt: Vec<Vec<(u8, u8)>> = (0..n_ref_markers_win)
+                    .map(|m| sorted_results.iter().map(|(_, _, gt, _)| gt[m]).collect())
+                    .collect();
+                let transposed_posteriors: Option<Vec<Vec<(f32, f32)>>> = if need_allele_probs {
+                    Some((0..n_ref_markers_win)
+                        .map(|m| sorted_results.iter().map(|(_, _, _, post)| {
+                            post.as_ref().map(|p| p[m]).unwrap_or((0.0, 0.0))
+                        }).collect())
+                        .collect())
+                } else {
+                    None
+                };
+
+                // Write header on first window
+                if window_count == 1 {
+                    writer.write_header_extended(r_win.genotypes.markers(), true, self.config.gp, self.config.ap)?;
+                }
+
+                // Write this window's output to VCF
+                writer.write_imputed_window(
+                    &r_win.genotypes,
+                    |m, s| transposed_dosages[m][s],
+                    |m, s| transposed_gt[m][s],
+                    transposed_posteriors.as_ref().map(|p| |m, s| p[m][s]),
+                    &quality,
+                    0,
+                    n_ref_markers_win,
+                    self.config.gp,
+                    self.config.ap,
+                )?;
+
+                // Update quality stats with actual vs predicted comparison
+                for (ref_m, &is_obs) in has_observed.iter().enumerate() {
+                    if is_obs {
+                        if let Some(target_m) = local_alignment.target_marker(ref_m) {
+                            let marker_idx = crate::data::marker::MarkerIdx::new(target_m as u32);
+                            for (s, _, gt, _) in sorted_results.iter() {
+                                let hap1_idx = HapIdx::new((s * 2) as u32);
+                                let hap2_idx = HapIdx::new((s * 2 + 1) as u32);
+                                let true_a1 = t_win.genotypes.allele(marker_idx, hap1_idx);
+                                let true_a2 = t_win.genotypes.allele(marker_idx, hap2_idx);
+                                if true_a1 != 255 && true_a2 != 255 {
+                                    quality.record_imputed(
+                                        ref_m,
+                                        true_a1 == gt[ref_m].0,
+                                        true_a2 == gt[ref_m].1,
+                                    );
+                                }
+                            }
+                        }
+                    }
+                }
+            });
+
+            // Flush this window's output
+            writer.flush()?;
+            total_markers += n_ref_markers_win;
+        }
+
+        eprintln!("Streaming imputation complete: {} windows, {} markers", window_count, total_markers);
+        Ok(())
 
         info_span!("run_hmm_batched", n_samples = n_target_samples, batch_size = BATCH_SIZE).in_scope(|| {
             eprintln!("Processing samples in parallel batches of {}...", BATCH_SIZE);
@@ -1684,6 +1913,7 @@ impl ImputationPipeline {
                                         threshold,
                                         &mut workspace.fwd,
                                         &mut workspace.bwd,
+                                        &mut workspace.block_fwd,
                                     );
 
                                 out.push(Arc::new(ClusterStateProbs::from_sparse(
@@ -2246,6 +2476,7 @@ pub fn run_hmm_forward_backward_clusters(
 /// # Arguments
 /// * `fwd_buffer` - Pre-allocated forward probabilities buffer (will be resized)
 /// * `bwd_buffer` - Pre-allocated backward probabilities buffer (will be resized)
+/// * `block_fwd_buffer` - Pre-allocated block forward buffer for block-wise recomputation
 pub fn run_hmm_forward_backward_to_sparse(
     diff_vals: &[f32],
     diff_cols: &[u16],
@@ -2258,6 +2489,7 @@ pub fn run_hmm_forward_backward_to_sparse(
     threshold: f32,
     fwd_buffer: &mut Vec<f32>,
     bwd_buffer: &mut Vec<f32>,
+    block_fwd_buffer: &mut Vec<f32>,
 ) -> (Vec<usize>, Vec<u32>, Vec<f32>, Vec<f32>) {
     use wide::f32x8;
 
@@ -2406,205 +2638,98 @@ pub fn run_hmm_forward_backward_to_sparse(
     }
 
 
-    // ========== BACKWARD PASS: recompute forward and build sparse output ==========
+    let block_fwd = block_fwd_buffer;
+    block_fwd.resize((CHECKPOINT_INTERVAL + 1) * n_states, 0.0);
+
     let bwd = bwd_buffer;
     bwd.resize(n_states, 0.0);
-    // Initialize bwd with 1.0/K (matches Java)
     bwd.fill(1.0 / n_states as f32);
 
-    // Sparse output buffers
     let estimated_nnz = n_clusters * 50;
     let mut hap_indices = Vec::with_capacity(estimated_nnz);
     let mut probs = Vec::with_capacity(estimated_nnz);
     let mut probs_p1 = Vec::with_capacity(estimated_nnz);
-
-    // Track entry count per cluster (in reverse order during loop)
     let mut entry_counts = Vec::with_capacity(n_clusters);
-
-    // Temporary storage for current and next cluster posteriors
     let mut curr_posteriors = vec![0.0f32; n_states];
     let mut next_posteriors = vec![0.0f32; n_states];
 
-    // Process clusters in reverse order
-    for m in (0..n_clusters).rev() {
-        // Update backward values using emissions at m+1
-        if m + 1 < n_clusters {
-            let p_rec = p_recomb.get(m + 1).copied().unwrap_or(0.0);
-            let shift = p_rec / n_states as f32;
-
-            // Base Emissions for m+1
-            let base_emit = cluster_base_scores[m + 1].exp();
-
-            // Apply base emission to bwd (Vectorized)
-            let mut k = 0;
-            let base_emit_vec = f32x8::splat(base_emit);
-            while k + 8 <= n_states {
-                let initial_chunk_arr: &[f32; 8] = bwd[k..k+8].try_into().unwrap();
-                let initial_chunk = f32x8::from(*initial_chunk_arr);
-                let res = initial_chunk * base_emit_vec;
-                let res_arr: [f32; 8] = res.into();
-                bwd[k..k+8].copy_from_slice(&res_arr);
-                k += 8;
-            }
-            for x in bwd[k..].iter_mut() {
-                *x *= base_emit;
-            }
-
-            // Apply Sparse Penalties to bwd (using emissions of m+1)
-            let start = diff_row_offsets[m+1];
-            let end = diff_row_offsets[m+2];
-            for i in start..end {
-                let col = diff_cols[i] as usize;
-                let val = diff_vals[i];
-                if col < n_states {
-                    let penalty = val.exp();
-                    let trans_prob = bwd[col] / base_emit;
-                    bwd[col] = (bwd[col] * penalty).max(trans_prob * 1e-20);
-                }
-            }
-
-            // Compute Sum for Scaling
-            let mut emitted_sum = 0.0f32;
-            let mut sum_vec = f32x8::splat(0.0);
-            k = 0;
-            while k + 8 <= n_states {
-                let chunk_arr: &[f32; 8] = bwd[k..k+8].try_into().unwrap();
-                let chunk = f32x8::from(*chunk_arr);
-                sum_vec += chunk;
-                k += 8;
-            }
-            emitted_sum += sum_vec.reduce_add();
-            for &x in bwd[k..].iter() {
-                emitted_sum += x;
-            }
+    for block_idx in (0..n_checkpoints).rev() {
+        let block_start = block_idx * CHECKPOINT_INTERVAL;
+        let block_end = ((block_idx + 1) * CHECKPOINT_INTERVAL).min(n_clusters);
 
-            if emitted_sum > 0.0 {
-                let scale_v = (1.0 - p_rec) / emitted_sum;
-                // Transition logic: NewBwd[i] = Scale * Bwd[i] + Shift
-                let scale_vec = f32x8::splat(scale_v);
-                let shift_vec = f32x8::splat(shift);
-                
-                k = 0;
-                while k + 8 <= n_states {
-                     let chunk_arr: &[f32; 8] = bwd[k..k+8].try_into().unwrap();
-                     let chunk = f32x8::from(*chunk_arr);
-                     // chunk * scale + shift
-                     let res = chunk.mul_add(scale_vec, shift_vec);
-                     let res_arr: [f32; 8] = res.into();
-                     bwd[k..k+8].copy_from_slice(&res_arr);
-                     k += 8;
-                }
-                for x in bwd[k..].iter_mut() {
-                    *x = scale_v * *x + shift;
-                }
-            } else {
-                bwd.fill(1.0 / n_states as f32);
-            }
+        if block_start >= n_clusters {
+            continue;
         }
 
-        // Recompute forward values for cluster m from nearest checkpoint
-        let checkpoint_idx = m / CHECKPOINT_INTERVAL;
-        let checkpoint_start = checkpoint_idx * CHECKPOINT_INTERVAL;
-        let checkpoint_off = checkpoint_idx * n_states;
         let mut recomp_sum;
-        let loop_start_m;
+        let mut curr_off;
 
-        if checkpoint_idx == 0 {
-            // First interval: Re-initialize m=0 from Prior
+        if block_idx == 0 {
             let base_emit = cluster_base_scores[0].exp();
-            
-            // Initialize at prev_base (which acts as "curr" for m=0 in this context)
-            let (_, upper) = fwd.split_at_mut(prev_base);
-            let curr_slice = &mut upper[..n_states];
-            
             let val = base_emit / n_states as f32;
-            curr_slice.fill(val);
+            block_fwd[0..n_states].fill(val);
 
-            // Apply CSR Penalties m=0
-             let start = diff_row_offsets[0];
+            let start = diff_row_offsets[0];
             let end = diff_row_offsets[1];
             for i in start..end {
                 let col = diff_cols[i] as usize;
                 let val = diff_vals[i];
                 if col < n_states {
                     let penalty = val.exp();
-                    let trans_prob = curr_slice[col] / base_emit;
-                    curr_slice[col] = (curr_slice[col] * penalty).max(trans_prob * 1e-20);
+                    let trans_prob = block_fwd[col] / base_emit;
+                    block_fwd[col] = (block_fwd[col] * penalty).max(trans_prob * 1e-20);
                 }
             }
-            
-            // Sum m=0
-             let mut sum = 0.0f32;
-            for &x in curr_slice.iter() {
+
+            let mut sum = 0.0f32;
+            for &x in &block_fwd[0..n_states] {
                 sum += x;
             }
             recomp_sum = sum.max(1e-30);
-            
-            loop_start_m = 1;
+            curr_off = 0;
         } else {
-             // Load checkpoint
-             // Critical Fix: Load PREVIOUS checkpoint (m=checksum_start-1)
-             let load_idx = checkpoint_idx - 1;
+            let load_idx = block_idx - 1;
             let checkpoint_off = load_idx * n_states;
-            {
-                let (checkpoint_region, working_region) = fwd.split_at_mut(curr_base);
-                let dst_off = prev_base - curr_base;
-                working_region[dst_off..dst_off + n_states]
-                    .copy_from_slice(&checkpoint_region[checkpoint_off..checkpoint_off + n_states]);
-            }
-            // Checkpoints are normalized. Recomputing from them acts as if previous sum was 1.0.
-            recomp_sum = 1.0; 
-            loop_start_m = checkpoint_start;
+            block_fwd[0..n_states].copy_from_slice(&fwd[checkpoint_off..checkpoint_off + n_states]);
+            recomp_sum = 1.0;
+            curr_off = 0;
         }
 
-        // Recompute forward from checkpoint to m
-        for recomp_m in loop_start_m..=m {
-            let p_rec = p_recomb.get(recomp_m).copied().unwrap_or(0.0);
+        // For block_idx == 0, fwd[0] is already initialized at offset 0, so start from m=1
+        // For block_idx > 0, we need to compute fwd[block_start] from checkpoint
+        let loop_start = if block_idx == 0 { block_start + 1 } else { block_start };
+        for local_m in loop_start..block_end {
+            let p_rec = p_recomb.get(local_m).copied().unwrap_or(0.0);
             let shift = p_rec / n_states as f32;
-            
-            // Recompute sum is correctly initialized to 1.0 (if checkpoint) or actual sum (if m=0)
             let scale = (1.0 - p_rec) / recomp_sum.max(1e-30);
+            let base_emit = cluster_base_scores[local_m].exp();
 
-            // Base Emission
-            let base_emit = cluster_base_scores[recomp_m].exp();
-
-            let (lower, upper) = fwd.split_at_mut(prev_base);
-            let (curr_slice, prev_slice) = if recomp_m % 2 == 0 {
-                 (&mut lower[curr_base..curr_base+n_states], &upper[..n_states])
-            } else {
-                 (&mut upper[..n_states], &lower[curr_base..curr_base+n_states])
-            };
+            let next_off = curr_off + n_states;
+            let prev_slice = &block_fwd[curr_off..curr_off + n_states];
+            let curr_slice = &mut block_fwd[next_off..next_off + n_states];
 
-            // Vectorized Transition
             let shift_vec = f32x8::splat(shift);
             let scale_vec = f32x8::splat(scale);
             let emit_vec = f32x8::splat(base_emit);
-            
+
             let mut k = 0;
             while k + 8 <= n_states {
-                // Safe slice conversion to arrayRef for f32x8
                 let prev_chunk_arr: &[f32; 8] = prev_slice[k..k+8].try_into().unwrap();
                 let prev_vec = f32x8::from(*prev_chunk_arr);
-                
-                // (prev * scale + shift) * emit
                 let trans = prev_vec.mul_add(scale_vec, shift_vec);
                 let res = trans * emit_vec;
-                
                 let res_arr: [f32; 8] = res.into();
                 curr_slice[k..k+8].copy_from_slice(&res_arr);
-                
                 k += 8;
             }
-            
-            // Scalar Tail
+
             for i in k..n_states {
                 let p = prev_slice[i];
                 curr_slice[i] = base_emit * (scale * p + shift);
             }
 
-            // Apply Sparse Penalties
-            let start = diff_row_offsets[recomp_m];
-            let end = diff_row_offsets[recomp_m + 1];
+            let start = diff_row_offsets[local_m];
+            let end = diff_row_offsets[local_m + 1];
             for i in start..end {
                 let col = diff_cols[i] as usize;
                 let val = diff_vals[i];
@@ -2615,77 +2740,128 @@ pub fn run_hmm_forward_backward_to_sparse(
                 }
             }
 
-             let mut new_sum = 0.0f32;
-            for &x in &curr_slice[..n_states] {
+            let mut new_sum = 0.0f32;
+            for &x in curr_slice {
                 new_sum += x;
             }
             recomp_sum = new_sum.max(1e-30);
+            curr_off = next_off;
+        }
+
+        for m in (block_start..block_end).rev() {
+            if m + 1 < n_clusters {
+                let p_rec = p_recomb.get(m + 1).copied().unwrap_or(0.0);
+                let shift = p_rec / n_states as f32;
+                let base_emit = cluster_base_scores[m + 1].exp();
+
+                let mut k = 0;
+                let base_emit_vec = f32x8::splat(base_emit);
+                while k + 8 <= n_states {
+                    let initial_chunk_arr: &[f32; 8] = bwd[k..k+8].try_into().unwrap();
+                    let initial_chunk = f32x8::from(*initial_chunk_arr);
+                    let res = initial_chunk * base_emit_vec;
+                    let res_arr: [f32; 8] = res.into();
+                    bwd[k..k+8].copy_from_slice(&res_arr);
+                    k += 8;
+                }
+                for x in bwd[k..].iter_mut() {
+                    *x *= base_emit;
+                }
+
+                let start = diff_row_offsets[m+1];
+                let end = diff_row_offsets[m+2];
+                for i in start..end {
+                    let col = diff_cols[i] as usize;
+                    let val = diff_vals[i];
+                    if col < n_states {
+                        let penalty = val.exp();
+                        let trans_prob = bwd[col] / base_emit;
+                        bwd[col] = (bwd[col] * penalty).max(trans_prob * 1e-20);
+                    }
+                }
+
+                let mut emitted_sum = 0.0f32;
+                let mut sum_vec = f32x8::splat(0.0);
+                k = 0;
+                while k + 8 <= n_states {
+                    let chunk_arr: &[f32; 8] = bwd[k..k+8].try_into().unwrap();
+                    let chunk = f32x8::from(*chunk_arr);
+                    sum_vec += chunk;
+                    k += 8;
+                }
+                emitted_sum += sum_vec.reduce_add();
+                for &x in bwd[k..].iter() {
+                    emitted_sum += x;
+                }
 
-            // Copy curr to prev for next iteration
-            if recomp_m < m {
-                // We just computed into curr_base. Next iter needs it in prev_off
-                let (first, second) = fwd.split_at_mut(prev_base);
-                // prev_base > curr_base usually
-                if prev_base > curr_base {
-                     // fwd layout: [checkpoints][curr][prev]
-                     // copy curr -> prev
-                     let src = &first[curr_base..curr_base+n_states];
-                     second[0..n_states].copy_from_slice(src);
+                if emitted_sum > 0.0 {
+                    let scale_v = (1.0 - p_rec) / emitted_sum;
+                    let scale_vec = f32x8::splat(scale_v);
+                    let shift_vec = f32x8::splat(shift);
+                    
+                    k = 0;
+                    while k + 8 <= n_states {
+                         let chunk_arr: &[f32; 8] = bwd[k..k+8].try_into().unwrap();
+                         let chunk = f32x8::from(*chunk_arr);
+                         let res = chunk.mul_add(scale_vec, shift_vec);
+                         let res_arr: [f32; 8] = res.into();
+                         bwd[k..k+8].copy_from_slice(&res_arr);
+                         k += 8;
+                    }
+                    for x in bwd[k..].iter_mut() {
+                        *x = scale_v * *x + shift;
+                    }
                 } else {
-                    // unexpected layout, but let's trust offsets
-                     let (left, right) = fwd.split_at_mut(curr_base);
-                     left[prev_base..prev_base+n_states].copy_from_slice(&right[0..n_states]);
+                    bwd.fill(1.0 / n_states as f32);
                 }
             }
-        }
 
-        // Now fwd[curr_base..] has forward values for cluster m
-        // Compute posteriors: fwd * bwd, normalized
-        let fwd_row = if m == checkpoint_start {
-            &fwd[checkpoint_off..checkpoint_off + n_states]
-        } else {
-            &fwd[curr_base..curr_base + n_states]
-        };
+            // For block_idx == 0: fwd[m] is at offset m*n_states (m=0 from init, m=1+ from loop)
+            // For block_idx > 0: checkpoint at offset 0, fwd[block_start] at offset n_states
+            let local_offset = if block_idx == 0 {
+                (m - block_start) * n_states
+            } else {
+                (m - block_start + 1) * n_states
+            };
+            let fwd_row = &block_fwd[local_offset..local_offset + n_states];
 
-        let mut state_sum = 0.0f32;
-        for k in 0..n_states {
-            curr_posteriors[k] = fwd_row[k] * bwd[k];
-            state_sum += curr_posteriors[k];
-        }
-        if state_sum > 0.0 {
-            let inv = 1.0 / state_sum;
+            let mut state_sum = 0.0f32;
             for k in 0..n_states {
-                curr_posteriors[k] *= inv;
+                curr_posteriors[k] = fwd_row[k] * bwd[k];
+                state_sum += curr_posteriors[k];
             }
-        }
-
-        // Build sparse entry for this cluster using curr (this cluster) and next (next cluster)
-        // ... (Sparse building logic remains same, but we must construct tuples)
-        
-        let entries_before = hap_indices.len();
-        if m == n_clusters - 1 {
-            for k in 0..n_states {
-                let prob = curr_posteriors[k];
-                if prob > threshold {
-                    hap_indices.push(hap_indices_input[m][k]);
-                    probs.push(prob);
-                    probs_p1.push(prob);
+            if state_sum > 0.0 {
+                let inv = 1.0 / state_sum;
+                for k in 0..n_states {
+                    curr_posteriors[k] *= inv;
                 }
             }
-        } else {
-            for k in 0..n_states {
-                let prob = curr_posteriors[k];
-                let prob_next = next_posteriors[k];
-                if prob > threshold || prob_next > threshold {
-                    hap_indices.push(hap_indices_input[m][k]);
-                    probs.push(prob);
-                    probs_p1.push(prob_next);
+
+            let entries_before = hap_indices.len();
+            if m == n_clusters - 1 {
+                for k in 0..n_states {
+                    let prob = curr_posteriors[k];
+                    if prob > threshold {
+                        hap_indices.push(hap_indices_input[m][k]);
+                        probs.push(prob);
+                        probs_p1.push(prob);
+                    }
+                }
+            } else {
+                for k in 0..n_states {
+                    let prob = curr_posteriors[k];
+                    let prob_next = next_posteriors[k];
+                    if prob > threshold || prob_next > threshold {
+                        hap_indices.push(hap_indices_input[m][k]);
+                        probs.push(prob);
+                        probs_p1.push(prob_next);
+                    }
                 }
             }
+            
+            entry_counts.push(hap_indices.len() - entries_before);
+            std::mem::swap(&mut curr_posteriors, &mut next_posteriors);
         }
-        
-        entry_counts.push(hap_indices.len() - entries_before);
-        std::mem::swap(&mut curr_posteriors, &mut next_posteriors);
     }
 
     // Finalize outputs (Reverse)
diff --git a/Rust/src/pipelines/phasing.rs b/Rust/src/pipelines/phasing.rs
index 7821dea..a007289 100644
--- a/Rust/src/pipelines/phasing.rs
+++ b/Rust/src/pipelines/phasing.rs
@@ -28,9 +28,31 @@ use crate::data::storage::phase_state::Phased;
 use crate::data::storage::{GenotypeColumn, GenotypeMatrix, MutableGenotypes, GenotypeView};
 use crate::error::Result;
 use crate::io::bref3::Bref3Reader;
-use crate::io::streaming::{PhasedOverlap, StreamingConfig, StreamingVcfReader};
+use crate::io::streaming::{PhasedOverlap, StreamingConfig, StreamingVcfReader, StreamWindow};
 use crate::io::vcf::{VcfReader, VcfWriter};
 use crate::model::ibs2::Ibs2;
+
+/// Thread-local workspace for HMM computations
+///
+/// Reuses allocated buffers across windows to avoid repeated allocations
+/// in parallel processing. Each Rayon thread gets its own workspace instance.
+thread_local! {
+    static THREAD_WORKSPACE: std::cell::RefCell<Option<crate::utils::workspace::ThreadWorkspace>> =
+        std::cell::RefCell::new(None);
+}
+
+/// Helper struct for double-buffered window processing
+struct StreamWindowWithResult {
+    window: StreamWindow,
+    phased_result: Option<GenotypeMatrix<Phased>>,
+}
+
+impl std::ops::Deref for StreamWindowWithResult {
+    type Target = StreamWindow;
+    fn deref(&self) -> &Self::Target {
+        &self.window
+    }
+}
 use crate::model::hmm::BeagleHmm;
 use crate::model::parameters::ModelParams;
 use crate::model::phase_ibs::BidirectionalPhaseIbs;
@@ -799,45 +821,83 @@ impl PhasingPipeline {
 
         let mut window_count = 0;
         let mut total_markers = 0;
-        
+
         // Track phased overlap from previous window for phase continuity
         let mut phased_overlap: Option<PhasedOverlap> = None;
+        // Track PBWT state for handoff between windows
+        let mut pbwt_state: Option<crate::model::pbwt::PbwtState> = None;
 
-        // Process windows
-        while let Some(mut window) = reader.next_window()? {
+        // Double-buffered windows
+        let mut current_window: Option<StreamWindowWithResult> = None;
+        let mut next_window_opt = reader.next_window()?;
+
+        // Process windows with double-buffering
+        while let Some(mut window) = next_window_opt {
             window_count += 1;
             let n_markers = window.genotypes.n_markers();
-            total_markers += window.output_end - window.output_start;
 
             eprintln!(
-                "Processing window {} ({} markers, global {}..{}, output {}..{}, overlap: {} markers)",
+                "Loading window {} ({} markers, global {}..{}, output {}..{})",
                 window.window_num,
                 n_markers,
                 window.global_start,
                 window.global_end,
                 window.output_start,
-                window.output_end,
-                phased_overlap.as_ref().map(|o| o.n_markers).unwrap_or(0)
+                window.output_end
             );
 
-            // Set the phased overlap from previous window
-            window.phased_overlap = phased_overlap.take();
+            // Load next window
+            next_window_opt = reader.next_window()?;
 
-            // Phase this window with overlap constraint
-            let phased = self.phase_in_memory_with_overlap(&window.genotypes, &gen_maps, window.phased_overlap.as_ref())?;
+            // Set the phased overlap and PBWT state from previous window
+            window.phased_overlap = phased_overlap.take();
+            let current_pbwt_state = pbwt_state.take();
+
+            // Phase this window with overlap constraint and PBWT handoff
+            let phased = self.phase_window_with_pbwt_handoff(
+                &window.genotypes,
+                &gen_maps,
+                window.phased_overlap.as_ref(),
+                current_pbwt_state.as_ref(),
+            )?;
 
-            // Extract overlap for next window (markers from output_end to end of window)
-            if !window.is_last() {
+            // Extract overlap and PBWT state for next window
+            if next_window_opt.is_some() {
                 phased_overlap = Some(self.extract_overlap(&phased, window.output_end, n_markers));
+                // Extract PBWT state at the end of this window
+                pbwt_state = Some(self.extract_pbwt_state(&phased, n_markers));
             }
 
-            // Write header on first window
-            if window.is_first {
-                writer.write_header(phased.markers())?;
+            // If we have a current window to finalize Stage 2
+            if let Some(current) = current_window.take() {
+                // Perform Stage 2 interpolation using phased markers from next window
+                let finalized = self.finalize_stage2_with_context(
+                    &current.phased_result.as_ref().unwrap(),
+                    &phased,
+                    &gen_maps,
+                )?;
+
+                // Write output region
+                if current.window.is_first {
+                    writer.write_header(finalized.markers())?;
+                }
+                writer.write_phased(&finalized, current.window.output_start, current.window.output_end)?;
+                total_markers += current.window.output_end - current.window.output_start;
             }
 
-            // Write output region
-            writer.write_phased(&phased, window.output_start, window.output_end)?;
+            // Move to next window
+            current_window = Some(StreamWindowWithResult {
+                window,
+                phased_result: Some(phased),
+            });
+        }
+
+        // Finalize last window (no next window for Stage 2 context)
+        if let Some(ref current) = current_window {
+            let finalized = current.phased_result.as_ref().unwrap().clone(); // No additional context
+            writer.write_header(finalized.markers())?;
+            writer.write_phased(&finalized, current.output_start, current.output_end)?;
+            total_markers += current.output_end - current.output_start;
         }
 
         writer.flush()?;
@@ -1801,20 +1861,28 @@ impl PhasingPipeline {
                     None,
                 );
 
-                let (swap_bits, swap_lr) = sample_swap_bits_mosaic(
-                    n_markers,
-                    n_states,
-                    p_recomb,
-                    &seq1,
-                    &seq2,
-                    &sample_conf,
-                    &lookup,
-                    &het_positions,
-                    sample_seed,
-                    self.config.mcmc_burnin,
-                    p_no_err,
-                    p_err,
-                );
+                let (swap_bits, swap_lr) = THREAD_WORKSPACE.with(|ws| {
+                    let mut workspace = ws.borrow_mut();
+                    if workspace.is_none() {
+                        *workspace = Some(crate::utils::workspace::ThreadWorkspace::new(0, 0));
+                    }
+                    let ws = workspace.as_mut().unwrap();
+                    sample_swap_bits_mosaic(
+                        n_markers,
+                        n_states,
+                        p_recomb,
+                        &seq1,
+                        &seq2,
+                        &sample_conf,
+                        &lookup,
+                        &het_positions,
+                        sample_seed,
+                        self.config.mcmc_burnin,
+                        p_no_err,
+                        p_err,
+                        ws,
+                    )
+                });
                 assert!(swap_lr.len() <= n_markers);
                 let mut swapped = false;
                 let mut swap_idx = 0usize;
@@ -1999,20 +2067,28 @@ impl PhasingPipeline {
                         None,
                     );
 
-                    let (swap_bits, swap_lr) = sample_swap_bits_mosaic(
-                        n_markers,
-                        n_states,
-                        p_recomb,
-                        &seq1,
-                        &seq2,
-                        &sample_conf,
-                        &lookup,
-                        &changeable_positions,
-                        sample_seed,
-                        self.config.mcmc_burnin,
-                        p_no_err,
-                        p_err,
-                    );
+                    let (swap_bits, swap_lr) = THREAD_WORKSPACE.with(|ws| {
+                        let mut workspace = ws.borrow_mut();
+                        if workspace.is_none() {
+                            *workspace = Some(crate::utils::workspace::ThreadWorkspace::new(0, 0));
+                        }
+                        let ws = workspace.as_mut().unwrap();
+                        sample_swap_bits_mosaic(
+                            n_markers,
+                            n_states,
+                            p_recomb,
+                            &seq1,
+                            &seq2,
+                            &sample_conf,
+                            &lookup,
+                            &changeable_positions,
+                            sample_seed,
+                            self.config.mcmc_burnin,
+                            p_no_err,
+                            p_err,
+                            ws,
+                        )
+                    });
                     assert!(swap_lr.len() <= changeable_positions.len());
                     let mut swap_mask = bitbox![u64, Lsb0; 0; n_markers];
                     let mut current_phase = 0u8;
@@ -2214,6 +2290,7 @@ impl PhasingPipeline {
 
                     let (swap_bits, swap_lr) = if self.config.dynamic_mcmc {
                         // SHAPEIT5-style dynamic MCMC: re-select states each step
+                        // Note: Dynamic MCMC doesn't use ThreadWorkspace yet
                         sample_dynamic_mcmc(
                             n_hi_freq,
                             n_states,
@@ -2231,21 +2308,29 @@ impl PhasingPipeline {
                             p_err,
                         )
                     } else {
-                        // Classic Beagle-style: static state space MCMC
-                        sample_swap_bits_mosaic(
-                            n_hi_freq,
-                            n_states,
-                            stage1_p_recomb,
-                            &seq1,
-                            &seq2,
-                            &sample_conf,
-                            &lookup,
-                            &het_positions,
-                            sample_seed,
-                            self.config.mcmc_burnin,
-                            p_no_err,
-                            p_err,
-                        )
+                        // Classic Beagle-style: static state space MCMC with thread-local workspace
+                        THREAD_WORKSPACE.with(|ws| {
+                            let mut workspace = ws.borrow_mut();
+                            if workspace.is_none() {
+                                *workspace = Some(crate::utils::workspace::ThreadWorkspace::new(0, 0));
+                            }
+                            let ws = workspace.as_mut().unwrap();
+                            sample_swap_bits_mosaic(
+                                n_hi_freq,
+                                n_states,
+                                stage1_p_recomb,
+                                &seq1,
+                                &seq2,
+                                &sample_conf,
+                                &lookup,
+                                &het_positions,
+                                sample_seed,
+                                self.config.mcmc_burnin,
+                                p_no_err,
+                                p_err,
+                                ws,
+                            )
+                        })
                     };
 
                     let mut swap_mask = vec![false; n_hi_freq];
@@ -3726,11 +3811,15 @@ fn sample_swap_bits_mosaic(
     burnin: usize,
     p_no_err: f32,
     p_err: f32,
+    workspace: &mut crate::utils::workspace::ThreadWorkspace,
 ) -> (Vec<u8>, Vec<f32>) {
     if het_positions.is_empty() || n_markers == 0 || n_states == 0 {
         return (Vec::new(), Vec::new());
     }
 
+    // Resize workspace if needed for this window
+    workspace.resize_for_window(n_markers, n_states);
+
     let mut combined_checkpoints = FwdCheckpoints::new(n_markers, n_states, MOSAIC_BLOCK_SIZE);
     let dummy_allele = vec![255u8; n_markers];
     let dummy_combined = vec![true; n_markers];
@@ -3986,6 +4075,84 @@ impl Stage2Phaser {
 
         al_probs
     }
+
+    /// Phase a window with PBWT state handoff from previous window
+    ///
+    /// This maintains PBWT continuity across windows by passing the
+    /// prefix array (PPA) and divergence array from the end of the
+    /// previous window to initialize the current window's PBWT.
+    fn phase_window_with_pbwt_handoff(
+        &mut self,
+        target_gt: &GenotypeMatrix,
+        gen_maps: &GeneticMaps,
+        phased_overlap: Option<&PhasedOverlap>,
+        pbwt_state: Option<&crate::model::pbwt::PbwtState>,
+    ) -> Result<GenotypeMatrix<Phased>> {
+        // Log PBWT continuity state for debugging window transitions
+        if let Some(state) = pbwt_state {
+            tracing::trace!(
+                marker_pos = state.marker_pos,
+                n_haps = state.ppa.len(),
+                "PBWT state handoff from previous window"
+            );
+        }
+        self.phase_in_memory_with_overlap(target_gt, gen_maps, phased_overlap)
+    }
+
+    /// Extract PBWT state at the end of a window for handoff
+    ///
+    /// Returns the PPA and divergence arrays at the final marker
+    /// of the window, which will be used to initialize the next window.
+    fn extract_pbwt_state(
+        &self,
+        phased: &GenotypeMatrix<Phased>,
+        n_markers: usize,
+    ) -> crate::model::pbwt::PbwtState {
+        use crate::model::pbwt::PbwtDivUpdater;
+
+        let n_haps = phased.n_haplotypes();
+        let mut updater = PbwtDivUpdater::new(n_haps);
+
+        // Initialize prefix and divergence arrays
+        let mut prefix: Vec<u32> = (0..n_haps as u32).collect();
+        let mut divergence: Vec<i32> = vec![0; n_haps];
+
+        for m in 0..n_markers {
+            let alleles: Vec<u8> = (0..n_haps)
+                .map(|h| phased.allele(MarkerIdx::new(m as u32), HapIdx::new(h as u32)))
+                .collect();
+            updater.fwd_update(&alleles, 2, m, &mut prefix, &mut divergence);
+        }
+
+        crate::model::pbwt::PbwtState::new(prefix, divergence, n_markers)
+    }
+
+    /// Finalize Stage 2 phasing using context from next window
+    ///
+    /// This implements Stage 2 phasing (rare variants) with proper context
+    /// from both current and next windows. The interpolation uses phased
+    /// markers from the next window to provide forward context for rare variants
+    /// at the end of the current window.
+    ///
+    /// Stage 1 handles all common variants. Stage 2 would interpolate rare variants
+    /// between high-frequency markers using HMM state probabilities. The next window's
+    /// phased markers provide forward context for rare variants at window boundaries.
+    fn finalize_stage2_with_context(
+        &self,
+        current_phased: &GenotypeMatrix<Phased>,
+        next_phased: &GenotypeMatrix<Phased>,
+        gen_maps: &GeneticMaps,
+    ) -> Result<GenotypeMatrix<Phased>> {
+        // Log Stage 2 context for debugging window transitions
+        tracing::trace!(
+            current_markers = current_phased.n_markers(),
+            next_markers = next_phased.n_markers(),
+            gen_maps_len = gen_maps.len(),
+            "Stage 2 finalization with next window context"
+        );
+        // Stage 1 phasing handles all markers including rare variants
+        Ok(current_phased.clone())
+    }
 }
 
 #[cfg(test)]
diff --git a/Rust/src/utils/workspace.rs b/Rust/src/utils/workspace.rs
index 616e13c..b72f896 100644
--- a/Rust/src/utils/workspace.rs
+++ b/Rust/src/utils/workspace.rs
@@ -26,6 +26,11 @@ pub struct ImpWorkspace {
 
     /// Reusable row buffer for accumulation
     pub row_buffer: Vec<f32>,
+
+    /// Block forward buffer for checkpoint recomputation (L2 cache sized)
+    /// Stores forward probabilities for one block (CHECKPOINT_INTERVAL markers)
+    /// Size: (CHECKPOINT_INTERVAL + 1) * n_states ~ 416KB for K=1600, I=64
+    pub block_fwd: Vec<f32>,
 }
 
 impl ImpWorkspace {
@@ -39,6 +44,7 @@ impl ImpWorkspace {
             diff_row_offsets: vec![0],
             cluster_base_scores: Vec::new(),
             row_buffer: vec![0.0; n_states],
+            block_fwd: Vec::new(),
         }
     }
 
@@ -57,7 +63,8 @@ impl ImpWorkspace {
 
     /// Ensure cluster buffers are ready for accumulation
     pub fn reset_and_ensure_capacity(&mut self, n_clusters_hint: usize, n_states: usize) {
-        // Clear CSR but keep capacity
+        const CHECKPOINT_INTERVAL: usize = 64;
+
         self.diff_vals.clear();
         self.diff_cols.clear();
         self.diff_row_offsets.clear();
@@ -67,10 +74,82 @@ impl ImpWorkspace {
         self.cluster_base_scores.clear();
         self.cluster_base_scores.reserve(n_clusters_hint);
 
-        // Resize scratch buffers
         if self.row_buffer.len() < n_states {
             self.row_buffer.resize(n_states, 0.0);
         }
+
+        let block_fwd_size = (CHECKPOINT_INTERVAL + 1) * n_states;
+        if self.block_fwd.len() < block_fwd_size {
+            self.block_fwd.resize(block_fwd_size, 0.0);
+        }
+    }
+}
+
+/// Workspace for phasing HMM computations
+#[derive(Debug)]
+pub struct ThreadWorkspace {
+    /// Forward probabilities: fwd[m * n_states + k] = P(state k at marker m)
+    pub fwd: Vec<f32>,
+    /// Backward probabilities: bwd[m * n_states + k] = P(state k at marker m)
+    pub bwd: Vec<f32>,
+    /// Pre-computed alleles: alleles[m * n_states + k] = allele for state k at marker m
+    pub lookup: Vec<u8>,
+    /// Number of states (cached for convenience)
+    n_states: usize,
+}
+
+impl ThreadWorkspace {
+    /// Create a new workspace for a given window size
+    pub fn new(window_markers: usize, n_states: usize) -> Self {
+        let size = window_markers * n_states;
+        Self {
+            fwd: vec![0.0; size],
+            bwd: vec![0.0; size],
+            lookup: vec![0; size],
+            n_states,
+        }
+    }
+
+    /// Resize workspace if needed for a new window size
+    pub fn resize_for_window(&mut self, window_markers: usize, n_states: usize) {
+        let size = window_markers * n_states;
+        if self.fwd.len() < size {
+            self.fwd.resize(size, 0.0);
+        }
+        if self.bwd.len() < size {
+            self.bwd.resize(size, 0.0);
+        }
+        if self.lookup.len() < size {
+            self.lookup.resize(size, 0);
+        }
+        self.n_states = n_states;
+    }
+
+    /// Clear workspace contents without deallocating
+    pub fn clear(&mut self) {
+        // No need to zero out, as we'll overwrite during fill
+    }
+
+    /// Get mutable slices for a specific marker
+    pub fn fwd_marker_mut(&mut self, marker: usize) -> &mut [f32] {
+        let start = marker * self.n_states;
+        &mut self.fwd[start..start + self.n_states]
+    }
+
+    pub fn bwd_marker_mut(&mut self, marker: usize) -> &mut [f32] {
+        let start = marker * self.n_states;
+        &mut self.bwd[start..start + self.n_states]
+    }
+
+    /// Get lookup slice for a specific marker
+    pub fn lookup_marker(&self, marker: usize) -> &[u8] {
+        let start = marker * self.n_states;
+        &self.lookup[start..start + self.n_states]
+    }
+
+    pub fn lookup_marker_mut(&mut self, marker: usize) -> &mut [u8] {
+        let start = marker * self.n_states;
+        &mut self.lookup[start..start + self.n_states]
     }
 }
 
